"""
í†µí•© ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ì—ì´ì „íŠ¸ ë§¤ë‹ˆì € - ë©€í‹°í„´/ì‹±ê¸€í†¤ ëŒ€í™” ì‹œìŠ¤í…œ
"""

import logging
import asyncio
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from enum import Enum
import json
from datetime import datetime

# ë©”ì‹œì§€ íƒ€ì… ì •ì˜
from typing import Dict, Any, List, Optional, Tuple, Literal

# ë©”ì‹œì§€ ì—­í•  íƒ€ì…
MessageRole = Literal["system", "user", "assistant"]

# ê³µí†µ ëª¨ë“ˆ ì„í¬íŠ¸
from shared_modules import (
    get_config,
    get_llm_manager,
    get_vector_manager,
    get_or_create_conversation_session,
    create_message,
    get_recent_messages,
    insert_message_raw,
    get_session_context,
    create_success_response,
    create_error_response,
    create_business_response,
    get_current_timestamp,
    format_conversation_history,
    load_prompt_from_file,
    PromptTemplate,
    get_template_by_title
)

from buisness_planning_agent.config.persona_config import PERSONA_CONFIG, get_persona_by_topic
from buisness_planning_agent.config.prompts_config import PROMPT_META
from buisness_planning_agent.utils.business_utils import (
    format_topic_prompts,
    format_business_summary,
    get_business_topics,
    get_info_gathering_questions,
    extract_business_info_with_llm,
    validate_conversation_completeness,
    generate_single_turn_response,
    create_conversation_state_summary,
)

logger = logging.getLogger(__name__)

class ConversationStage(Enum):
    """ëŒ€í™” ë‹¨ê³„ ì •ì˜"""
    INITIAL = "initial"                    # ì´ˆê¸° ì ‘ì´‰
    INFORMATION_GATHERING = "info_gathering"  # ì •ë³´ ìˆ˜ì§‘
    ANALYSIS = "analysis"                  # ë¶„ì„
    PLANNING = "planning"                  # ê¸°íš/ê³„íš ìˆ˜ë¦½
    PROPOSAL = "proposal"                  # ì œì•ˆ
    FEEDBACK = "feedback"                  # í”¼ë“œë°± ìˆ˜ì§‘
    REFINEMENT = "refinement"              # ìˆ˜ì •
    FINAL_RESULT = "final_result"          # ìµœì¢… ê²°ê³¼
    COMPLETED = "completed"                # ì™„ë£Œ

class ConversationState:
    """ëŒ€í™” ìƒíƒœ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, conversation_id: int, user_id: int):
        self.conversation_id = conversation_id
        self.user_id = user_id
        self.stage = ConversationStage.INITIAL
        self.created_at = datetime.now()
        self.updated_at = datetime.now()
        
        # ìˆ˜ì§‘ëœ ì •ë³´
        self.collected_info = {
            "business_idea": None,           # ë¹„ì¦ˆë‹ˆìŠ¤ ì•„ì´ë””ì–´
            "industry": None,                # ì—…ì¢…/ì‚°ì—…
            "target_customers": None,        # íƒ€ê²Ÿ ê³ ê°
            "unique_value": None,            # ê³ ìœ  ê°€ì¹˜/ì°¨ë³„ì 
            "business_stage": None,          # í˜„ì¬ ë‹¨ê³„ (ì•„ì´ë””ì–´/ì¤€ë¹„/ìš´ì˜ ë“±)
            "budget": None,                  # ì˜ˆì‚°
            "timeline": None,                # íƒ€ì„ë¼ì¸
            "location": None,                # ì§€ì—­/ìœ„ì¹˜
            "team_size": None,               # íŒ€ ê·œëª¨
            "experience": None,              # ê´€ë ¨ ê²½í—˜
            "goals": None,                   # ëª©í‘œ
            "concerns": None,                # ìš°ë ¤ì‚¬í•­
            "additional_context": {}
        }
        
        # ë¶„ì„ ê²°ê³¼
        self.analysis_results = {
            "primary_topics": [],
            "intent_analysis": {},
            "market_analysis": {},
            "risk_analysis": {},
            "recommendations": []
        }
        
        # ê³„íš ë° ì œì•ˆ
        self.business_plans = []
        self.feedback_history = []
        self.refinements = []
        
        # ìµœì¢… ê²°ê³¼
        self.final_plan = None
        self.action_plan = None
        
        # ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ê¸°ë¡
        self.stage_prompts = {}
        
    def update_stage(self, new_stage: ConversationStage):
        """ë‹¨ê³„ ì—…ë°ì´íŠ¸"""
        self.stage = new_stage
        self.updated_at = datetime.now()
        
    def add_collected_info(self, key: str, value: Any):
        """ìˆ˜ì§‘ëœ ì •ë³´ ì¶”ê°€"""
        self.collected_info[key] = value
        self.updated_at = datetime.now()
        
    def add_feedback(self, feedback: Dict[str, Any]):
        """í”¼ë“œë°± ì¶”ê°€"""
        feedback["timestamp"] = datetime.now()
        self.feedback_history.append(feedback)
        self.updated_at = datetime.now()
        
    def is_information_complete(self) -> bool:
        """ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€ í™•ì¸"""
        required_fields = ["business_idea", "industry", "target_customers", "goals"]
        return all(self.collected_info.get(field) for field in required_fields)
        
    def get_completion_rate(self) -> float:
        """ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œìœ¨"""
        total_fields = len(self.collected_info)
        completed_fields = len([v for v in self.collected_info.values() if v])
        return completed_fields / total_fields if total_fields > 0 else 0.0

class BusinessPlanningAgentManager:
    """í†µí•© ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ì—ì´ì „íŠ¸ ê´€ë¦¬ì - ë©€í‹°í„´/ì‹±ê¸€í†¤ ëŒ€í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.config = get_config()
        self.llm_manager = get_llm_manager()
        self.vector_manager = get_vector_manager()
        
        # í”„ë¡¬í”„íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.prompts_dir = Path(__file__).parent.parent / 'prompts'
        
        # ì „ë¬¸ ì§€ì‹ ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
        self.knowledge_collection = 'business-planning-knowledge'
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš í† í”½ ë° ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
        self.business_topics = get_business_topics()
        self.info_gathering_questions = get_info_gathering_questions()
        
        # ëŒ€í™” ìƒíƒœ ê´€ë¦¬ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        self.conversation_states: Dict[int, ConversationState] = {}
        
        # ì§€ì‹ ê¸°ë°˜ ì´ˆê¸°í™”
        self._initialize_knowledge_base()
    
    def _initialize_knowledge_base(self):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ì „ë¬¸ ì§€ì‹ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        try:
            vectorstore = self.vector_manager.get_vectorstore(
                collection_name=self.knowledge_collection,
                create_if_not_exists=True
            )
            
            if not vectorstore:
                logger.warning("ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return
            
            logger.info("âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ì „ë¬¸ ì§€ì‹ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì „ë¬¸ ì§€ì‹ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def get_or_create_conversation_state(self, conversation_id: int, user_id: int) -> ConversationState:
        """ëŒ€í™” ìƒíƒœ ì¡°íšŒ ë˜ëŠ” ìƒì„±"""
        if conversation_id not in self.conversation_states:
            self.conversation_states[conversation_id] = ConversationState(conversation_id, user_id)
        return self.conversation_states[conversation_id]
    
    def load_topic_prompt(self, topic: str) -> str:
        """í† í”½ë³„ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì§ì ‘ ë¡œë“œ"""
        try:
            prompt_file = self.prompts_dir / f"{topic}.txt"
            if prompt_file.exists():
                with open(prompt_file, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                logger.warning(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì—†ìŒ: {topic}")
                return ""
        except Exception as e:
            logger.error(f"í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ({topic}): {e}")
            return ""
    
    def classify_business_topic_with_llm(self, user_input: str, context: str = "") -> List[str]:
        """LLMì„ í™œìš©í•œ ë¹„ì¦ˆë‹ˆìŠ¤ í† í”½ ë¶„ë¥˜"""
        try:
            topic_classification_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš í† í”½ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ í† í”½:
{chr(10).join([f"- {key}: {value}" for key, value in self.business_topics.items()])}

{f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸: {context}" if context else ""}

ì‚¬ìš©ì ì§ˆë¬¸: "{user_input}"

ìœ„ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ í† í”½ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ì„ íƒí•˜ì—¬ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì˜ˆì‹œ: startup_preparation, idea_validation, business_model

ë‹µë³€:"""

            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ì „ë¬¸ê°€ë¡œì„œ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì •í™•í•œ ë¹„ì¦ˆë‹ˆìŠ¤ í† í”½ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤."},
                {"role": "user", "content": topic_classification_prompt}
            ]
            
            response = self.llm_manager.generate_response_sync(messages)
            
            if response:
                topics = [topic.strip() for topic in response.split(',')]
                valid_topics = [topic for topic in topics if topic in self.business_topics]
                return valid_topics[:3] if valid_topics else ["startup_preparation"]
            
            return ["startup_preparation"]
            
        except Exception as e:
            logger.error(f"LLM í† í”½ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return ["startup_preparation", "business_model"]
    
    def analyze_user_intent_and_stage_with_llm(self, user_input: str, state: ConversationState) -> Dict[str, Any]:
        """LLMì„ í™œìš©í•œ ì‚¬ìš©ì ì˜ë„ ë° ëŒ€í™” ë‹¨ê³„ ë¶„ì„"""
        try:
            context_info = {
                "current_stage": state.stage.value,
                "collected_info": state.collected_info,
                "completion_rate": state.get_completion_rate()
            }
            
            intent_analysis_prompt = f"""ì‚¬ìš©ìì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ìƒë‹´ ì˜ë„ì™€ ëŒ€í™” ì§„í–‰ ë°©í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

í˜„ì¬ ëŒ€í™” ìƒíƒœ:
- ë‹¨ê³„: {state.stage.value}
- ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œìœ¨: {state.get_completion_rate():.1%}
- ìˆ˜ì§‘ëœ ì •ë³´: {json.dumps(state.collected_info, ensure_ascii=False, indent=2)}

ì‚¬ìš©ì ì…ë ¥: "{user_input}"

ë‹¤ìŒ JSON í˜•íƒœë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "intent_type": "info_provide|question_ask|feedback_give|refinement_request|completion_request|general_inquiry|lean_canvas_request",
    "confidence": 0.9,
    "extracted_info": {{
        "field_name": "ì¶”ì¶œëœ ì •ë³´ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)"
    }},
    "next_stage_recommendation": "info_gathering|analysis|planning|proposal|feedback|refinement|final_result",
    "user_sentiment": "positive|neutral|negative|frustrated",
    "urgency_level": "high|medium|low",
    "suggested_questions": ["ì¶”ê°€ë¡œ ë¬¼ì–´ë³¼ ì§ˆë¬¸ë“¤"],
    "business_focus_areas": ["ë¶„ì„í•´ì•¼ í•  ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì—­ë“¤"]
}}

ë¶„ì„ ê²°ê³¼:"""

            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ìƒë‹´ ì „ë¬¸ê°€ë¡œì„œ ëŒ€í™” íë¦„ê³¼ ì‚¬ìš©ì ì˜ë„ë¥¼ ì •í™•íˆ ë¶„ì„í•©ë‹ˆë‹¤."),
                HumanMessage(content=intent_analysis_prompt)
            ]
            
            response = self.llm_manager.generate_response_sync(messages, output_format="json")
            
            if isinstance(response, dict):
                return response
            elif isinstance(response, str):
                try:
                    return json.loads(response)
                except json.JSONDecodeError:
                    pass
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "intent_type": "general_inquiry",
                "confidence": 0.7,
                "extracted_info": {},
                "next_stage_recommendation": "info_gathering",
                "user_sentiment": "neutral",
                "urgency_level": "medium",
                "suggested_questions": [],
                "business_focus_areas": []
            }
            
        except Exception as e:
            logger.error(f"LLM ì˜ë„ ë° ë‹¨ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "intent_type": "general_inquiry",
                "confidence": 0.5,
                "extracted_info": {},
                "next_stage_recommendation": "info_gathering",
                "user_sentiment": "neutral",
                "urgency_level": "medium",
                "suggested_questions": [],
                "business_focus_areas": []
            }
    
    def extract_business_information_with_llm(self, user_input: str, current_info: Dict[str, Any]) -> Dict[str, Any]:
        """LLMì„ í™œìš©í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        return extract_business_info_with_llm(self.llm_manager, user_input, current_info, self.info_gathering_questions)
    
    def handle_information_gathering(self, user_input: str, state: ConversationState, intent_analysis: Dict[str, Any]) -> str:
        """ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ ì²˜ë¦¬"""
        
        # LLM ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
        extracted_info = self.extract_business_information_with_llm(user_input, state.collected_info)
        
        # ìˆ˜ì§‘ëœ ì •ë³´ ì—…ë°ì´íŠ¸
        for field, value in extracted_info.items():
            if field in state.collected_info and value:
                state.add_collected_info(field, value)
        
        # ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ í™•ì¸
        completion_check = validate_conversation_completeness(state.collected_info, self.info_gathering_questions)
        
        if completion_check["is_complete"] or completion_check["completion_rate"] >= 0.7:
            # ë¶„ì„ ë‹¨ê³„ë¡œ ì „í™˜
            state.update_stage(ConversationStage.ANALYSIS)
            return self._generate_transition_to_analysis_response(state)
        
        # ë‹¤ìŒ ì •ë³´ ìˆ˜ì§‘ ì§ˆë¬¸ ìƒì„±
        missing_fields = completion_check["missing_fields"]
        next_question = self._generate_next_question_with_llm(user_input, state, missing_fields)
        
        return next_question
    
    def _generate_next_question_with_llm(self, user_input: str, state: ConversationState, missing_fields: List[str]) -> str:
        """LLM ê¸°ë°˜ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±"""
        try:
            collected_summary = format_business_summary(state.collected_info, self.info_gathering_questions)
            
            question_prompt = f"""ì‚¬ìš©ìì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´ ìˆ˜ì§‘ì„ ìœ„í•œ ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

í˜„ì¬ ìˆ˜ì§‘ëœ ì •ë³´:
{collected_summary}

ì•„ì§ í•„ìš”í•œ ì •ë³´ í•­ëª©ë“¤:
{chr(10).join([f"- {field}: {self.info_gathering_questions.get(field, field)}" for field in missing_fields[:3]])}

ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë‹µë³€: "{user_input}"

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
1. ìˆ˜ì§‘ëœ ì •ë³´ì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½ê³¼ ê°ì‚¬ ì¸ì‚¬
2. ê°€ì¥ ì¤‘ìš”í•œ ë‹¤ìŒ ì§ˆë¬¸ 1ê°œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì œì‹œ
3. ì™œ ì´ ì •ë³´ê°€ í•„ìš”í•œì§€ ê°„ë‹¨í•œ ì„¤ëª… í¬í•¨

ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸ë¡œì„œ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ìœ¼ë¡œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."},
                {"role": "user", "content": question_prompt}
            ]
            
            response = self.llm_manager.generate_response_sync(messages)
            return response if response else "ë‹¤ìŒ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
            
        except Exception as e:
            logger.error(f"ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            if missing_fields:
                field = missing_fields[0]
                return f"ğŸ’¡ **ë‹¤ìŒ ì§ˆë¬¸**: {self.info_gathering_questions.get(field, field)}\n\në” ì •í™•í•œ ë§ì¶¤ ë¹„ì¦ˆë‹ˆìŠ¤ ê³„íšì„ ìœ„í•´ ìœ„ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”!"
            return "ì¶”ê°€ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
    
    def _generate_transition_to_analysis_response(self, state: ConversationState) -> str:
        """ë¶„ì„ ë‹¨ê³„ë¡œ ì „í™˜ ì‹œ ì‘ë‹µ ìƒì„±"""
        summary = format_business_summary(state.collected_info, self.info_gathering_questions)
        
        return f"""ğŸ¯ **ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ!** 

ìˆ˜ì§‘ëœ ì •ë³´:
{summary}

ì´ì œ ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì‹¬ì¸µ ë¶„ì„**ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. 
ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... ğŸ“Š"""
    
    def handle_analysis_stage(self, user_input: str, state: ConversationState) -> str:
        """ë¶„ì„ ë‹¨ê³„ ì²˜ë¦¬"""
        
        try:
            # ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í† í”½ ë¶„ë¥˜
            analysis_context = " ".join([str(v) for v in state.collected_info.values() if v])
            topics = self.classify_business_topic_with_llm(analysis_context)
            
            # í† í”½ë³„ í”„ë¡¬í”„íŠ¸ ì§ì ‘ ë¡œë“œ
            topic_prompts = {}
            for topic in topics:
                prompt_content = self.load_topic_prompt(topic)
                if prompt_content:
                    topic_prompts[topic] = prompt_content
            
            # ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰
            relevant_knowledge = self.get_relevant_knowledge(analysis_context, topics)
            
            # LLM ê¸°ë°˜ ë¶„ì„ ìˆ˜í–‰
            analysis_result = self._perform_llm_analysis(state, topics, topic_prompts, relevant_knowledge)
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            state.analysis_results = {
                "primary_topics": topics,
                "topic_prompts_used": list(topic_prompts.keys()),
                "analysis_content": analysis_result,
                "timestamp": datetime.now()
            }
            
            # ê¸°íš ë‹¨ê³„ë¡œ ì „í™˜
            state.update_stage(ConversationStage.PLANNING)
            
            return f"""ğŸ” **ì‹¬ì¸µ ë¶„ì„ ì™„ë£Œ**

{analysis_result}

---
ì´ì œ ì´ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ **êµ¬ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê³„íšê³¼ ì‹¤í–‰ ì „ëµ**ì„ ìˆ˜ë¦½í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”! ğŸš€"""
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ë‹¨ê³„ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _perform_llm_analysis(self, state: ConversationState, topics: List[str], topic_prompts: Dict[str, str], relevant_knowledge: List[str]) -> str:
        """LLM ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ìˆ˜í–‰"""
        try:
            analysis_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ë¬¸ê°€ ê´€ì ì—ì„œ ì‹¬ì¸µ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

ìˆ˜ì§‘ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´:
{json.dumps(state.collected_info, ensure_ascii=False, indent=2)}

ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ í† í”½: {', '.join(topics)}

í† í”½ë³„ ë¶„ì„ ì§€ì¹¨:
{format_topic_prompts(topic_prompts, self.business_topics)}

ì „ë¬¸ ì§€ì‹ ì°¸ê³ :
{chr(10).join(relevant_knowledge) if relevant_knowledge else "ê¸°ë³¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì§€ì‹ í™œìš©"}

ìœ„ í† í”½ë³„ ì§€ì¹¨ì— ë”°ë¼ ë‹¤ìŒ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. **ë¹„ì¦ˆë‹ˆìŠ¤ ì•„ì´ë””ì–´ ë¶„ì„**: ì•„ì´ë””ì–´ì˜ ê°•ì ê³¼ ê°œì„ ì 
2. **ì‹œì¥ ê¸°íšŒ ë¶„ì„**: ì‹œì¥ ìƒí™©ê³¼ ê¸°íšŒ ìš”ì†Œ
3. **ê²½ìŸë ¥ ë¶„ì„**: ì°¨ë³„í™” í¬ì¸íŠ¸ì™€ ê²½ìŸ ìš°ìœ„
4. **ë¦¬ìŠ¤í¬ ë¶„ì„**: ì˜ˆìƒ ìœ„í—˜ ìš”ì†Œì™€ ëŒ€ì‘ ë°©ì•ˆ
5. **ì‹¤í˜„ ê°€ëŠ¥ì„±**: í˜„ì‹¤ì ì¸ ì‹¤í–‰ ê°€ëŠ¥ì„± í‰ê°€
6. **ìš°ì„ ìˆœìœ„**: ê°€ì¥ ë¨¼ì € í•´ì•¼ í•  3ê°€ì§€

ì „ë¬¸ì ì´ë©´ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ì „ë¬¸ê°€ë¡œì„œ ì£¼ì–´ì§„ í† í”½ë³„ ì§€ì¹¨ì— ë”°ë¼ ë°ì´í„° ê¸°ë°˜ì˜ ì‹¤ìš©ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": analysis_prompt}
            ]
            
            return self.llm_manager.generate_response_sync(messages)
            
        except Exception as e:
            logger.error(f"LLM ë¶„ì„ ìˆ˜í–‰ ì‹¤íŒ¨: {e}")
            return "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def get_relevant_knowledge(self, query: str, topics: List[str] = None) -> List[str]:
        """ê´€ë ¨ ì „ë¬¸ ì§€ì‹ ê²€ìƒ‰"""
        try:
            search_results = self.vector_manager.search_documents(
                query=query,
                collection_name=self.knowledge_collection,
                k=5
            )
            
            knowledge_texts = []
            for doc in search_results[:3]:
                knowledge_area = doc.metadata.get('knowledge_area', 'ì¼ë°˜')
                content = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
                knowledge_texts.append(f"[{knowledge_area}]\n{content}")
            
            return knowledge_texts
            
        except Exception as e:
            logger.error(f"ì „ë¬¸ ì§€ì‹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def handle_lean_canvas_request(self, user_input: str, state: ConversationState) -> str:
        """ë¦°ìº”ë²„ìŠ¤ ìš”ì²­ ì²˜ë¦¬"""
        try:
            # LLM ê¸°ë°˜ ë§ì¶¤í˜• í…œí”Œë¦¿ ì„ íƒ
            template_title = self._select_lean_canvas_template_with_llm(state.collected_info)
            
            # í…œí”Œë¦¿ ì¡°íšŒ
            template = get_template_by_title(template_title)
            
            if template:
                return f"""ğŸ“‹ **ë§ì¶¤í˜• ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿**

ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **{template_title}** í…œí”Œë¦¿ì„ ì¶”ì²œí•©ë‹ˆë‹¤.

{template["content"]}

---
ì´ í…œí”Œë¦¿ì„ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì„ ì„¤ê³„í•´ë³´ì„¸ìš”!
ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ìˆ˜ì •ì´ í•„ìš”í•˜ë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."""
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            logger.error(f"ë¦°ìº”ë²„ìŠ¤ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return "ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _select_lean_canvas_template_with_llm(self, business_info: Dict[str, Any]) -> str:
        """LLM ê¸°ë°˜ ë§ì¶¤í˜• ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ ì„ íƒ"""
        try:
            template_selection_prompt = f"""ë‹¤ìŒ ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ì„ ì„ íƒí•´ì£¼ì„¸ìš”.

ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë³´:
{json.dumps(business_info, ensure_ascii=False, indent=2)}

ì‚¬ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿:
- ë¦° ìº”ë²„ìŠ¤_common: ì¼ë°˜ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸
- ë¦° ìº”ë²„ìŠ¤_nail: ë„¤ì¼ìƒµ/ë„¤ì¼ì•„íŠ¸ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤
- ë¦° ìº”ë²„ìŠ¤_eyelash: ì†ëˆˆì¹ ì—°ì¥/ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤
- ë¦° ìº”ë²„ìŠ¤_ecommers: ì‡¼í•‘ëª°/ì´ì»¤ë¨¸ìŠ¤ ë¹„ì¦ˆë‹ˆìŠ¤
- ë¦° ìº”ë²„ìŠ¤_creator: ìœ íŠœë²„/í¬ë¦¬ì—ì´í„° ë¹„ì¦ˆë‹ˆìŠ¤

ê°€ì¥ ì í•©í•œ í…œí”Œë¦¿ ì´ë¦„ë§Œ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ì „ë¬¸ê°€ë¡œì„œ ì£¼ì–´ì§„ ì •ë³´ì— ê°€ì¥ ì í•©í•œ ë¦°ìº”ë²„ìŠ¤ í…œí”Œë¦¿ì„ ì„ íƒí•©ë‹ˆë‹¤."},
                {"role": "user", "content": template_selection_prompt}
            ]
            
            response = self.llm_manager.generate_response_sync(messages)
            
            # ìœ íš¨í•œ í…œí”Œë¦¿ì¸ì§€ í™•ì¸
            valid_templates = [
                "ë¦° ìº”ë²„ìŠ¤_common", "ë¦° ìº”ë²„ìŠ¤_nail", "ë¦° ìº”ë²„ìŠ¤_eyelash", 
                "ë¦° ìº”ë²„ìŠ¤_ecommers", "ë¦° ìº”ë²„ìŠ¤_creator"
            ]
            
            for template in valid_templates:
                if template in response:
                    return template
            
            return "ë¦° ìº”ë²„ìŠ¤_common"  # ê¸°ë³¸ê°’
            
        except Exception as e:
            logger.error(f"LLM ê¸°ë°˜ í…œí”Œë¦¿ ì„ íƒ ì‹¤íŒ¨: {e}")
            return "ë¦° ìº”ë²„ìŠ¤_common"
    
    def handle_single_turn_request(self, user_input: str, user_id: int) -> str:
        """ì‹±ê¸€í†¤ ìš”ì²­ ì²˜ë¦¬"""
        try:
            return generate_single_turn_response(
                llm_manager=self.llm_manager,
                user_input=user_input,
                business_topics=self.business_topics,
                classify_func=self.classify_business_topic_with_llm,
                load_prompt_func=self.load_topic_prompt,
                get_knowledge_func=self.get_relevant_knowledge
            )
        except Exception as e:
            logger.error(f"ì‹±ê¸€í†¤ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return "ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def determine_conversation_mode(self, user_input: str, conversation_id: Optional[int]) -> str:
        """ëŒ€í™” ëª¨ë“œ ê²°ì • (ì‹±ê¸€í†¤ vs ë©€í‹°í„´)"""
        try:
            # conversation_idê°€ Noneì´ë©´ ì‹±ê¸€í†¤
            if conversation_id is None:
                return "single_turn"
            
            # ê¸°ì¡´ ëŒ€í™” ìƒíƒœê°€ ìˆìœ¼ë©´ ë©€í‹°í„´
            if conversation_id in self.conversation_states:
                return "multi_turn"
            
            # LLM ê¸°ë°˜ ëª¨ë“œ íŒë‹¨
            mode_analysis_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ìƒë‹´ ë°©ì‹ì„ ê²°ì •í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{user_input}"

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- single_turn: ê°„ë‹¨í•œ ì§ˆë¬¸ì´ë‚˜ ì¦‰ì„ ë‹µë³€ì´ ê°€ëŠ¥í•œ ê²½ìš°
- multi_turn: ì²´ê³„ì ì¸ ìƒë‹´ì´ë‚˜ ì •ë³´ ìˆ˜ì§‘ì´ í•„ìš”í•œ ê²½ìš°

íŒë‹¨ ê¸°ì¤€:
- ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ê³„íš ìƒë‹´ â†’ multi_turn
- ê°„ë‹¨í•œ ì •ë³´ ë¬¸ì˜ë‚˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ â†’ single_turn
- ë¦°ìº”ë²„ìŠ¤ ìš”ì²­ â†’ single_turn (ì¦‰ì‹œ ì œê³µ ê°€ëŠ¥)
- ê°œì¸ ë§ì¶¤ ë¶„ì„ ìš”ì²­ â†’ multi_turn

ë‹µë³€:"""

            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ íŒë‹¨í•˜ì—¬ ì ì ˆí•œ ìƒë‹´ ë°©ì‹ì„ ê²°ì •í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": mode_analysis_prompt}
            ]
            
            response = self.llm_manager.generate_response_sync(convert_messages_to_dict(messages))
            
            if "single_turn" in response.lower():
                return "single_turn"
            elif "multi_turn" in response.lower():
                return "multi_turn"
            else:
                # ê¸°ë³¸ê°’: ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼
                return "multi_turn"
                
        except Exception as e:
            logger.error(f"ëŒ€í™” ëª¨ë“œ ê²°ì • ì‹¤íŒ¨: {e}")
            return "multi_turn"  # ê¸°ë³¸ê°’
    
    def process_user_query(
        self, 
        user_input: str, 
        user_id: int, 
        conversation_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ - ë©€í‹°í„´/ì‹±ê¸€í†¤ ì§€ì›"""
        
        try:
            logger.info(f"ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: {user_input[:50]}...")
            
            # ëŒ€í™” ëª¨ë“œ ê²°ì •
            conversation_mode = self.determine_conversation_mode(user_input, conversation_id)
            
            if conversation_mode == "single_turn":
                # ì‹±ê¸€í†¤ ì²˜ë¦¬
                response_content = self.handle_single_turn_request(user_input, user_id)
                
                return create_business_response(
                    conversation_id=None,
                    answer=response_content,
                    topics=self.classify_business_topic_with_llm(user_input),
                    sources="ì‹±ê¸€í†¤ ì‘ë‹µ ì‹œìŠ¤í…œ"
                )
            
            else:
                # ë©€í‹°í„´ ì²˜ë¦¬
                return self._handle_multi_turn_conversation(user_input, user_id, conversation_id)
            
        except Exception as e:
            logger.error(f"ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return create_error_response(
                error_message=f"ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ìƒë‹´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                error_code="BUSINESS_PLANNING_ERROR"
            )
    
    def _handle_multi_turn_conversation(self, user_input: str, user_id: int, conversation_id: Optional[int]) -> Dict[str, Any]:
        """ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬"""
        # ëŒ€í™” ì„¸ì…˜ ì²˜ë¦¬
        session_info = get_or_create_conversation_session(user_id, conversation_id)
        conversation_id = session_info["conversation_id"]
        
        # ëŒ€í™” ìƒíƒœ ì¡°íšŒ/ìƒì„±
        state = self.get_or_create_conversation_state(conversation_id, user_id)
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        with get_session_context() as db:
            create_message(db, conversation_id, "user", "business_planning", user_input)
        
        # ë¦°ìº”ë²„ìŠ¤ ìš”ì²­ ì²´í¬
        if "ë¦°ìº”ë²„ìŠ¤" in user_input:
            response_content = self.handle_lean_canvas_request(user_input, state)
        else:
            # ì˜ë„ ë° ë‹¨ê³„ ë¶„ì„
            intent_analysis = self.analyze_user_intent_and_stage_with_llm(user_input, state)
            
            # í˜„ì¬ ë‹¨ê³„ì— ë”°ë¥¸ ì²˜ë¦¬
            if state.stage == ConversationStage.INITIAL:
                # ì´ˆê¸° ì ‘ì´‰ ì‹œ ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ë¡œ ì „í™˜
                state.update_stage(ConversationStage.INFORMATION_GATHERING)
                response_content = f"""ì•ˆë…•í•˜ì„¸ìš”! 1ì¸ ì°½ì—… ì „ë¬¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ğŸš€

ë§ì¶¤í˜• ë¹„ì¦ˆë‹ˆìŠ¤ ê³„íšì„ ìˆ˜ë¦½í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ì§ˆë¬¸ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

**ì²« ë²ˆì§¸ ì§ˆë¬¸**: {list(self.info_gathering_questions.values())[0]}

ì²´ê³„ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê³„íš ìˆ˜ë¦½ì„ ìœ„í•´ ì°¨ê·¼ì°¨ê·¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤!"""
                
            elif state.stage == ConversationStage.INFORMATION_GATHERING:
                response_content = self.handle_information_gathering(user_input, state, intent_analysis)
                
            elif state.stage == ConversationStage.ANALYSIS:
                response_content = self.handle_analysis_stage(user_input, state)
                
            # ì¶”ê°€ ë‹¨ê³„ë“¤ì€ í•„ìš”ì‹œ êµ¬í˜„
            else:
                response_content = "ì£„ì†¡í•©ë‹ˆë‹¤. ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì€ ë‹¨ê³„ì…ë‹ˆë‹¤."
        
        # ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
        insert_message_raw(
            conversation_id=conversation_id,
            sender_type="agent",
            agent_type="business_planning",
            content=response_content
        )
        
        # í‘œì¤€ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        return create_business_response(
            conversation_id=conversation_id,
            answer=response_content,
            topics=getattr(state.analysis_results, 'primary_topics', []),
            sources=f"ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ (ë‹¨ê³„: {state.stage.value})"
        )
    
    def get_agent_status(self) -> Dict[str, Any]:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê¸°íš ì—ì´ì „íŠ¸ ìƒíƒœ ë°˜í™˜"""
        return {
            "agent_type": "business_planning",
            "version": "4.0.0",
            "conversation_system": "multi_turn_and_single_turn",
            "stages": [stage.value for stage in ConversationStage],
            "active_conversations": len(self.conversation_states),
            "conversation_stages": {
                conv_id: state.stage.value 
                for conv_id, state in self.conversation_states.items()
            },
            "llm_status": self.llm_manager.get_status(),
            "vector_store_status": self.vector_manager.get_status()
        }
