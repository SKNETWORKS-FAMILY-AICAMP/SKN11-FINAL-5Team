import mcp
from mcp.client.streamable_http import streamablehttp_client
import asyncio
import json
from config import SMITHERY_API_KEY
from datetime import datetime
import re
import base64

#  SMITHERY_API_KEY = e94738a7-0a18-43a1-9674-8378d161b423
bright_data_url = (
    f"https://server.smithery.ai/@luminati-io/brightdata-mcp/mcp"
    f"?api_key={SMITHERY_API_KEY}&profile=blank-tahr-7BPYtb"
)

def get_season(month):
    if month in [3, 4, 5]:      return "ë´„"
    elif month in [6, 7, 8]:    return "ì—¬ë¦„"
    elif month in [9, 10, 11]:  return "ê°€ì„"
    else:                       return "ê²¨ìš¸"


async def get_trending_nail_trend(url, max_results=2):
    answer = []
    async with streamablehttp_client(url) as (read_stream, write_stream, _):
        async with mcp.ClientSession(read_stream, write_stream) as session:
            await session.initialize()
            today = datetime.now()
            season = get_season(today.month)
            query = f"{today.year}ë…„ {season} ë„¤ì¼ íŠ¸ë Œë“œ"
            params = {"query": query, "engine": "google"}
            search_result = await session.call_tool("search_engine", params)
            text = search_result.content[0].text

            url_pattern = re.compile(r"\(http[^)]+\)")
            url_candidates = url_pattern.findall(text)
            links = [s[1:-1] for s in url_candidates]

            try:
                links = links[6:] # êµ¬ê¸€ ë¶€ê°€ë§í¬ ì œì™¸
                if len(links) < 1:
                    raise ValueError
            except Exception:
                return "\n[!] URL ì¶”ì¶œì„ ìœ„í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.\nê²€ìƒ‰ì–´ë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¾¸ê±°ë‚˜, ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„í•´ ì£¼ì„¸ìš”."

            # 2. ë³¸ë¬¸ ìˆ˜ì§‘ (ì—ëŸ¬ ì—†ëŠ” 2ê°œê¹Œì§€)
            collected_texts = []
            count = 0
            for url_ in links:
                if count >= max_results:
                    break
                try:
                    scrape_result = await session.call_tool("scrape_as_markdown", {"url": url_})
                    if not getattr(scrape_result, "isError", False):
                        content = (scrape_result.content[0].text or '').strip()
                        collected_texts.append(content)
                        count += 1
                except Exception as e:
                    continue

            if not collected_texts:
                return "[!] ë³¸ë¬¸ ì¶”ì¶œì— ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì´ë‚˜ í‚¤ì›Œë“œë¥¼ ë°”ê¿”ë³´ì„¸ìš”."

            all_content = "-----------------------------".join(collected_texts)
            all_content = re.sub(r'!\[.*?\]\([^)]+\)', '', all_content)
            all_content = re.sub(r'\(http[^)]+\)', '', all_content)
            all_content = re.sub(r'\n+', ' ', all_content)
            answer.append(all_content)
    return "\n".join(answer)


if __name__ == "__main__":
    nail_trend = asyncio.run(get_trending_nail_trend(bright_data_url))
  

    # ì´ê±¸ gptì—ê²Œ ë„˜ê¸°ê¸°
    print("\nğŸ’¡ ë„¤ì¼ íŠ¸ë Œë“œ\n", nail_trend) 
   