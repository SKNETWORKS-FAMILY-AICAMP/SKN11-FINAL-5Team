"""
í™˜ê²½ ì„¤ì • ë° LLM í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ (OpenAI â†’ Gemini fallback)
"""
import os
import logging
from typing import Dict, Any, Optional, Union
from dotenv import load_dotenv

# config/env_config.py

from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# OpenAI LLM ì„¤ì •
llm = ChatOpenAI(
    model_name="gpt-4", 
    temperature=0.3
)

# ë²¡í„°ìŠ¤í† ì–´ (ì˜ˆ: ChromaDB) ì„¤ì •
embedding = OpenAIEmbeddings()

# ì˜ˆ: Chroma vector store ë¡œë“œ
vectorstore = Chroma(
    collection_name="global-documents", 
    embedding_function=embedding,
    persist_directory="./vector_db_text-3-small_pdf"
)


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

logger = logging.getLogger(__name__)

class LLMConfig:
    """LLM ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.gemini_api_key = os.getenv('GEMINI_API_KEY')
        self.current_provider = None
        self.openai_client = None
        self.gemini_client = None
        
    def get_available_providers(self) -> list:
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM í”„ë¡œë°”ì´ë” ëª©ë¡ ë°˜í™˜"""
        providers = []
        if self.openai_api_key:
            providers.append('openai')
        if self.gemini_api_key:
            providers.append('gemini')
        return providers
    
    def initialize_openai(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            import openai
            self.openai_client = openai.OpenAI(api_key=self.openai_api_key)
            logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            return True
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def initialize_gemini(self):
        """Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_client = genai.GenerativeModel('gemini-pro')
            logger.info("Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            return True
        except Exception as e:
            logger.error(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

# ì „ì—­ LLM ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
_llm_config = LLMConfig()

class LLMClient:
    """LLM í´ë¼ì´ì–¸íŠ¸ (ìë™ fallback ì§€ì›)"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.current_provider = None
        self.initialize_clients()
    
    def initialize_clients(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìš°ì„ ìˆœìœ„: OpenAI â†’ Gemini)"""
        # OpenAI ë¨¼ì € ì‹œë„
        if self.config.openai_api_key and self.config.initialize_openai():
            self.current_provider = 'openai'
            logger.info("Primary LLM: OpenAI")
            return
        
        # OpenAI ì‹¤íŒ¨ ì‹œ Gemini ì‹œë„
        if self.config.gemini_api_key and self.config.initialize_gemini():
            self.current_provider = 'gemini'
            logger.info("Primary LLM: Gemini (fallback)")
            return
        
        logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ LLMì´ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    def generate_response(self, messages: list, **kwargs) -> str:
        """ì‘ë‹µ ìƒì„± (ìë™ fallback)"""
        # 1ì°¨ ì‹œë„: í˜„ì¬ í”„ë¡œë°”ì´ë”
        try:
            if self.current_provider == 'openai':
                return self._generate_openai(messages, **kwargs)
            elif self.current_provider == 'gemini':
                return self._generate_gemini(messages, **kwargs)
        except Exception as e:
            logger.warning(f"{self.current_provider} ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            
        # 2ì°¨ ì‹œë„: fallback
        try:
            if self.current_provider == 'openai' and self.config.gemini_client:
                logger.info("Geminië¡œ fallback ì‹œë„")
                return self._generate_gemini(messages, **kwargs)
            elif self.current_provider == 'gemini' and self.config.openai_client:
                logger.info("OpenAIë¡œ fallback ì‹œë„")
                return self._generate_openai(messages, **kwargs)
        except Exception as e:
            logger.error(f"Fallbackë„ ì‹¤íŒ¨: {e}")
        
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ì ‘ì†í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _generate_openai(self, messages: list, **kwargs) -> str:
        """OpenAIë¡œ ì‘ë‹µ ìƒì„±"""
        if not self.config.openai_client:
            raise Exception("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        response = self.config.openai_client.chat.completions.create(
            model=kwargs.get('model', 'gpt-3.5-turbo'),
            messages=messages,
            max_tokens=kwargs.get('max_tokens', 1000),
            temperature=kwargs.get('temperature', 0.7)
        )
        return response.choices[0].message.content
    
    def _generate_gemini(self, messages: list, **kwargs) -> str:
        """Geminië¡œ ì‘ë‹µ ìƒì„±"""
        if not self.config.gemini_client:
            raise Exception("Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # messagesë¥¼ Gemini í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        prompt = self._convert_messages_to_prompt(messages)
        
        response = self.config.gemini_client.generate_content(
            prompt,
            generation_config={
                'max_output_tokens': kwargs.get('max_tokens', 1000),
                'temperature': kwargs.get('temperature', 0.7)
            }
        )
        return response.text
    
    def _convert_messages_to_prompt(self, messages: list) -> str:
        """OpenAI ë©”ì‹œì§€ í˜•ì‹ì„ Gemini í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
        prompt_parts = []
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            
            if role == 'system':
                prompt_parts.append(f"[ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­]\n{content}\n")
            elif role == 'user':
                prompt_parts.append(f"[ì‚¬ìš©ì]\n{content}\n")
            elif role == 'assistant':
                prompt_parts.append(f"[ì–´ì‹œìŠ¤í„´íŠ¸]\n{content}\n")
        
        return "\n".join(prompt_parts)
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ LLM ìƒíƒœ ë°˜í™˜"""
        return {
            'current_provider': self.current_provider,
            'available_providers': self.config.get_available_providers(),
            'openai_available': bool(self.config.openai_client),
            'gemini_available': bool(self.config.gemini_client)
        }

# ì „ì—­ LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
_llm_client = None

def get_llm_client() -> LLMClient:
    """LLM í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient(_llm_config)
    return _llm_client

def get_config() -> Dict[str, Any]:
    """ì „ì²´ í™˜ê²½ ì„¤ì • ë°˜í™˜"""
    return {
        'app_name': os.getenv('APP_NAME', 'Solo Preneur Helper'),
        'debug': os.getenv('DEBUG', 'False').lower() == 'true',
        'log_level': os.getenv('LOG_LEVEL', 'INFO'),
        'max_tokens': int(os.getenv('MAX_TOKENS', '1000')),
        'temperature': float(os.getenv('TEMPERATURE', '0.7')),
        'llm_config': {
            'available_providers': _llm_config.get_available_providers(),
            'primary_provider': 'openai' if _llm_config.openai_api_key else 'gemini'
        }
    }

def test_llm_connection():
    """LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
    client = get_llm_client()
    
    print("ğŸ” LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"í˜„ì¬ ìƒíƒœ: {client.get_status()}")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
    test_messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ì„ í•´ì£¼ì„¸ìš”."}
    ]
    
    try:
        response = client.generate_response(test_messages)
        print(f"âœ… LLM ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {response[:100]}...")
        return True
    except Exception as e:
        print(f"âŒ LLM ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    test_llm_connection()